{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0075a997",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "BoW and Tf-Idf are useful for topic detection, spam detection,... But when it comes to predict the next word they fail to capture the sentiment\n",
    "\n",
    "1. Assign each number a unique number to represent the word\n",
    "\n",
    "This will cause a fail prediction, we can have (happy = 1, bad = 30, good = 31), so good is closer to bad than happy, thus good = bad, which is not what we wanted\n",
    "\n",
    "2. One hot, represent each word as vector of 0, and mark 1 to the corresponding position. So it don't care the problem of assigning numbers\n",
    "\n",
    "This is eat up a lot of memory unecessary, because this vector could be very large (consider the BoW), and still fail to capture the context\n",
    "\n",
    "3. Word2Vec, suppose we have a vector of feature for certain word, then we can measure all other word in that vector\n",
    "\n",
    "E.g. bird = [animal,can_fly][1,1] fox = [animal,cannot_fly][1,0].\n",
    "\n",
    "So in the example we put the words into a 2D dimension, but this could easily take more than 2 dimensions.\n",
    "\n",
    "This representation will capture their meaning, how?\n",
    "\n",
    "E.g. Beijing - China + Japan = Tokyo, why? Think Beijing as capital of China, then we toggle China and add Japan, becoming capital of Japan which is Tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd08e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random, math\n",
    "\n",
    "corpus = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"i love dogs and i love cats\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the fox is quick and the fox is clever\",\n",
    "    \"cats and dogs are animals\",\n",
    "    \"the bird can fly\",\n",
    "    \"a fox is a wild animal\",\n",
    "    \"birds can fly in the sky\",\n",
    "    \"dogs bark and cats meow\",\n",
    "    \"the quick bird flew over the lazy cat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf16961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 10, 'fox': 4, 'and': 4, 'quick': 3, 'dogs': 3, 'cats': 3, 'is': 3, 'over': 2, 'lazy': 2, 'dog': 2, 'i': 2, 'love': 2, 'cat': 2, 'bird': 2, 'can': 2, 'fly': 2, 'a': 2, 'brown': 1, 'jumps': 1, 'chased': 1, 'clever': 1, 'are': 1, 'animals': 1, 'wild': 1, 'animal': 1, 'birds': 1, 'in': 1, 'sky': 1, 'bark': 1, 'meow': 1, 'flew': 1})\n",
      "\n",
      "Example pairs (center -> context):\n",
      "the -> quick\n",
      "the -> brown\n",
      "quick -> the\n",
      "quick -> brown\n",
      "quick -> fox\n",
      "Epoch 50, Loss=524.3849\n",
      "Epoch 100, Loss=437.3974\n",
      "Epoch 150, Loss=374.7613\n",
      "Epoch 200, Loss=330.7061\n",
      "Epoch 250, Loss=313.7642\n",
      "Epoch 300, Loss=310.8752\n",
      "Epoch 350, Loss=295.2842\n",
      "Epoch 400, Loss=292.6350\n",
      "Epoch 450, Loss=298.0102\n",
      "Epoch 500, Loss=295.5132\n",
      "Epoch 550, Loss=275.0126\n",
      "Epoch 600, Loss=294.2135\n",
      "Epoch 650, Loss=282.3212\n",
      "Epoch 700, Loss=280.0014\n",
      "Epoch 750, Loss=279.9481\n",
      "Epoch 800, Loss=280.4740\n",
      "Epoch 850, Loss=287.4539\n",
      "Epoch 900, Loss=301.5977\n",
      "Epoch 950, Loss=278.2768\n",
      "Epoch 1000, Loss=293.7246\n",
      "\n",
      "Nearest neighbors for 'dog': [('cat', np.float64(0.9970804281416459)), ('over', np.float64(0.6865253888473178)), ('chased', np.float64(0.6588969376151362)), ('sky', np.float64(0.6057888677158793)), ('brown', np.float64(0.5792896586621867))]\n",
      "Nearest neighbors for 'fox': [('wild', np.float64(0.6383191992968955)), ('brown', np.float64(0.637811278292)), ('is', np.float64(0.565462119613098)), ('jumps', np.float64(0.563323681605174)), ('flew', np.float64(0.5459177665520252))]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text) -> list:\n",
    "    text = text.lower() # lowercase\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "out = []\n",
    "for phrase in corpus:\n",
    "    out+=tokenize(phrase)\n",
    "\n",
    "counter = Counter(out)\n",
    "print(counter)\n",
    "\n",
    "# Create dicts\n",
    "vocab = counter.keys()\n",
    "sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "w2i = {token: idx for idx, (token, _) in enumerate(sorted_tokens)}\n",
    "i2w = {idx: token for idx, (token, _) in enumerate(sorted_tokens)}\n",
    "\n",
    "# Skip-gram\n",
    "window_size = 2\n",
    "pairs = []\n",
    "for phrase in corpus:\n",
    "    tokens = tokenize(phrase)\n",
    "    idxs = [w2i[w] for w in tokens]\n",
    "\n",
    "    for i, center in enumerate(idxs):\n",
    "        for w in range(-window_size, window_size+1):\n",
    "            j = i + w\n",
    "            if w == 0 or j < 0 or j >= len(idxs):\n",
    "                continue\n",
    "            pairs.append((center, idxs[j]))\n",
    "\n",
    "print(\"\\nExample pairs (center -> context):\")\n",
    "for i in range(5):\n",
    "    print(i2w[pairs[i][0]], \"->\", i2w[pairs[i][1]])\n",
    "\n",
    "n = len(sorted_tokens)\n",
    "# Sample the negative (those words that never appeared with center)\n",
    "# Instead of softmax on millions\n",
    "freq = np.array([counter[i2w[i]] for i in range(n)], dtype=np.float64)\n",
    "unigram = freq ** 0.75 # Avoid common word dominance, not sampling uniform\n",
    "unigram /= unigram.sum() # normalized\n",
    "\n",
    "embedding_dim = 25\n",
    "# Embedding when its center word (hidden layer)\n",
    "W_in = (np.random.rand(n,embedding_dim) - 0.5) / embedding_dim\n",
    "# Embedding when its context word (output layer)\n",
    "W_out = (np.random.rand(n,embedding_dim) - 0.5) / embedding_dim\n",
    "\n",
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)\n",
    "\n",
    "epochs = 1000\n",
    "alpha = 0.01\n",
    "neg_sample = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    random.shuffle(pairs) # Don't let it memorize pairs\n",
    "\n",
    "    for center,context in pairs:\n",
    "        center_vec = W_in[center]\n",
    "        output_vec = W_out[context]\n",
    "\n",
    "        # Positive update\n",
    "        score = sigmoid(np.dot(center_vec,output_vec))\n",
    "        loss = -np.log(score + 1e-10)\n",
    "        grad = alpha * (1-score)\n",
    "\n",
    "        W_in[center] += grad * output_vec\n",
    "        W_out[context] += grad * center_vec\n",
    "\n",
    "        loss_sum += loss\n",
    "\n",
    "        # Negative update\n",
    "        negatives = np.random.choice(n,size=neg_sample,p=unigram)\n",
    "        for neg in negatives:\n",
    "            if neg == context:\n",
    "                continue\n",
    "            output_k = W_out[neg]\n",
    "            score_neg = sigmoid(np.dot(center_vec,output_k))\n",
    "            loss = -np.log(1 - score_neg + 1e-10)\n",
    "            grad = alpha * (0 - score_neg)\n",
    "\n",
    "            W_in[center] += grad * output_k\n",
    "            W_out[neg] += grad * center_vec\n",
    "\n",
    "            loss_sum += loss\n",
    "        \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        # Every 50 epochs print loss\n",
    "        print(f\"Epoch {epoch+1}, Loss={loss_sum:.4f}\")\n",
    "\n",
    "embeddings = W_in\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    return np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
    "\n",
    "def nearest(word,topn = 5):\n",
    "    if word not in w2i:\n",
    "        return []\n",
    "    idx = w2i[word]\n",
    "    vec = embeddings[idx]\n",
    "    sims = [cosine_similarity(vec,embeddings[i]) for i in range(n)]\n",
    "    best = np.argsort(sims)[::-1][1:topn+1]\n",
    "    return [(i2w[i], sims[i]) for i in best]\n",
    "\n",
    "print(\"\\nNearest neighbors for 'dog':\", nearest(\"dog\"))\n",
    "print(\"Nearest neighbors for 'fox':\", nearest(\"fox\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0e07fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'king', 's', 'crown', 'shining', 'bright', 'in', 'love', 'and', 'death']\n",
      "Counter({'e': 14960, 't': 11863, 'o': 11218, 'a': 9950, 'h': 8731, 'i': 8511, 's': 8379, 'n': 8297, 'r': 7777, 'l': 5847, 'd': 5025, 'u': 4343, 'm': 4253, 'y': 3204, 'w': 3132, 'f': 2698, 'c': 2606, 'g': 2420, 'p': 2016, 'b': 1830, 'k': 1272, 'v': 1222, 'q': 220, 'x': 179, 'j': 110, 'z': 72})\n",
      "Original tokens: 130135\n",
      "Filtered tokens: 1920\n",
      "\n",
      "Example pairs (center -> context):\n",
      "a -> h\n",
      "a -> h\n",
      "h -> a\n",
      "h -> h\n",
      "h -> e\n",
      "Epoch 50, Loss=22680.7540\n",
      "Epoch 100, Loss=24457.0781\n",
      "Epoch 150, Loss=57229.7477\n",
      "Epoch 200, Loss=119028.9095\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # Only keep alphabetic words, drop punctuation/numbers\n",
    "    tokens = re.findall(r\"[a-z]+\", text)\n",
    "    return tokens\n",
    "\n",
    "sample = tokenize(\"The king's crown, shining bright in love and death.\")\n",
    "print(sample)\n",
    "\n",
    "out = []\n",
    "for phrase in corpus:\n",
    "    out+=tokenize(phrase)\n",
    "\n",
    "counter = Counter(out)\n",
    "print(counter)\n",
    "\n",
    "# Create dicts\n",
    "vocab = counter.keys()\n",
    "sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "w2i = {token: idx for idx, (token, _) in enumerate(sorted_tokens)}\n",
    "i2w = {idx: token for idx, (token, _) in enumerate(sorted_tokens)}\n",
    "\n",
    "# Downsample, since its too large\n",
    "total_count = sum(counter.values())\n",
    "threshold = 1e-5\n",
    "\n",
    "freq = np.array([counter[i2w[i]] for i in range(len(w2i))], dtype=np.float64)\n",
    "prob = freq / total_count\n",
    "\n",
    "# P(discard word) = 1 - sqrt(t / f)\n",
    "discard_prob = 1 - np.sqrt(threshold / prob)\n",
    "discard_prob = np.clip(discard_prob, 0, 1)  # ensure [0,1]\n",
    "\n",
    "filtered_tokens = []\n",
    "for w in out:\n",
    "    if w not in w2i:   # safeguard\n",
    "        continue\n",
    "    if np.random.rand() > discard_prob[w2i[w]]:\n",
    "        filtered_tokens.append(w)\n",
    "\n",
    "\n",
    "print(\"Original tokens:\", len(out))\n",
    "print(\"Filtered tokens:\", len(filtered_tokens))\n",
    "\n",
    "tokens = filtered_tokens  \n",
    "# Skip-gram\n",
    "window_size = 2\n",
    "pairs = []\n",
    "\n",
    "idxs = [w2i[w] for w in filtered_tokens]  # use filtered_tokens instead of corpus\n",
    "for i, center in enumerate(idxs):\n",
    "    for w in range(-window_size, window_size + 1):\n",
    "        j = i + w\n",
    "        if w == 0 or j < 0 or j >= len(idxs):\n",
    "            continue\n",
    "        pairs.append((center, idxs[j]))\n",
    "\n",
    "print(\"\\nExample pairs (center -> context):\")\n",
    "for i in range(min(5, len(pairs))):\n",
    "    print(i2w[pairs[i][0]], \"->\", i2w[pairs[i][1]])\n",
    "\n",
    "n = len(sorted_tokens)\n",
    "# Sample the negative (those words that never appeared with center)\n",
    "# Instead of softmax on millions\n",
    "freq = np.array([counter[i2w[i]] for i in range(n)], dtype=np.float64)\n",
    "unigram = freq ** 0.75 # Avoid common word dominance, not sampling uniform\n",
    "unigram /= unigram.sum() # normalized\n",
    "\n",
    "embedding_dim = 50 # Increased\n",
    "# Embedding when its center word (hidden layer)\n",
    "W_in = (np.random.rand(n,embedding_dim) - 0.5) / embedding_dim\n",
    "# Embedding when its context word (output layer)\n",
    "W_out = (np.random.rand(n,embedding_dim) - 0.5) / embedding_dim\n",
    "\n",
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)\n",
    "\n",
    "epochs = 200\n",
    "alpha = 0.01\n",
    "neg_sample = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    random.shuffle(pairs) # Don't let it memorize pairs\n",
    "\n",
    "    for center,context in pairs:\n",
    "        center_vec = W_in[center]\n",
    "        output_vec = W_out[context]\n",
    "\n",
    "        # Positive update\n",
    "        score = sigmoid(np.dot(center_vec,output_vec))\n",
    "        loss = -np.log(score + 1e-10)\n",
    "        grad = alpha * (1-score)\n",
    "\n",
    "        W_in[center] += grad * output_vec\n",
    "        W_out[context] += grad * center_vec\n",
    "\n",
    "        loss_sum += loss\n",
    "\n",
    "        # Negative update\n",
    "        negatives = np.random.choice(n,size=neg_sample,p=unigram)\n",
    "        for neg in negatives:\n",
    "            if neg == context:\n",
    "                continue\n",
    "            output_k = W_out[neg]\n",
    "            score_neg = sigmoid(np.dot(center_vec,output_k))\n",
    "            loss = -np.log(1 - score_neg + 1e-10)\n",
    "            grad = alpha * (0 - score_neg)\n",
    "\n",
    "            W_in[center] += grad * output_k\n",
    "            W_out[neg] += grad * center_vec\n",
    "\n",
    "            loss_sum += loss\n",
    "        \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        # Every 50 epochs print loss\n",
    "        print(f\"Epoch {epoch+1}, Loss={loss_sum:.4f}\")\n",
    "\n",
    "embeddings = W_in\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    return np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
    "\n",
    "def nearest(word,topn = 5):\n",
    "    if word not in w2i:\n",
    "        return []\n",
    "    idx = w2i[word]\n",
    "    vec = embeddings[idx]\n",
    "    sims = [cosine_similarity(vec,embeddings[i]) for i in range(n)]\n",
    "    best = np.argsort(sims)[::-1][1:topn+1]\n",
    "    return [(i2w[i], sims[i]) for i in best]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da85ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m vecs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([embeddings[w2i[w]] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m w2i])\n\u001b[0;32m     11\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m reduced \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvecs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n",
      "File \u001b[1;32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:466\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:503\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    495\u001b[0m     )\n\u001b[0;32m    497\u001b[0m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2952\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2954\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2956\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1091\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1085\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1086\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1087\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1088\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m             )\n\u001b[1;32m-> 1091\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1096\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1097\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "print(nearest(\"king\"))\n",
    "print(nearest(\"love\"))\n",
    "print(nearest(\"death\"))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"love\", \"death\", \"life\", \"hamlet\"]\n",
    "vecs = np.array([embeddings[w2i[w]] for w in words if w in w2i])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(vecs)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, w in enumerate(words):\n",
    "    if w in w2i:\n",
    "        x, y = reduced[i]\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x+0.01, y+0.01, w, fontsize=12)\n",
    "plt.title(\"Word2Vec Embeddings (PCA Projection)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
