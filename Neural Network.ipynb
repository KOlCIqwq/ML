{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab37fd6",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Neuron\n",
    "A neuron is a small component in the neural network, it computes a weighted sum of inputs plut a bias, and then applies it to an activation function\n",
    "\n",
    "So it should receive inputs and it will eventually given an ouput\n",
    "\n",
    "$$\n",
    "z = w^{T}x + b = \\sum_{i=1}^{n}w_{i}x_{i} + b\n",
    "$$\n",
    "here $w^{T}$ is the weight matrix and $x$ are the inputs (previous values)\n",
    "\n",
    "we can do a small example:\n",
    "$$\n",
    "x = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} w = \\begin{bmatrix} 0.5 \\\\ -1.0 \\end{bmatrix}, b = 0.1\\\\\n",
    "w_{1}x_{1} = 0.5 * 1 = 0.5\\\\\n",
    "w_{2}x_{2} = -1.0 * 2 = -2.0\\\\\n",
    "\\sum{}{} = 0.5 + (-2.0) = -1.5\\\\\n",
    "add bias: z = -1.5 + 0.1 = -1.4\\\\\n",
    "ReLU a = max(0,z) = 0\\\\\n",
    "$$\n",
    "\n",
    "### Activation function\n",
    "It's a function that calculates the output of the node based on its individual inputs and theri weights\n",
    "\n",
    "Previously we used in logistic regression the sigmoid activation function\n",
    "\n",
    "Recalling it: $\\sigma(x) = \\frac{1}{1+e^{-x}}$, but there are other activation functions\n",
    "\n",
    "$ReLU = max(0,z)$ very effective and simple in these neural network\n",
    "\n",
    "$$\n",
    "Softmax = \\frac{e^x_{i}}{\\sum_{j=1}^{J}e^x_{j}}\n",
    "$$ \n",
    "usually used in the categorial last layer outputs, forming percentage for each prediction\n",
    "\n",
    "### Layers\n",
    "The first layer is called input layer, which nodes don't don't operations, and usually size of inputs\n",
    "\n",
    "The last layer is the output layer, usually size of the desidered output vector size\n",
    "\n",
    "For example in the well-known number recognition, the input layer is the size of the image (length*width), and the output layer is 10 to get the vector of probability for each number\n",
    "\n",
    "The layer between these 2 are called hidden layer, since they could be any size, now for a hidden layer $l$ it will have a weighted matrix $W^{l} \\in \\mathbb{R}^{n}$ (m neurons, n inputs), a bias vector (1 hot) $b^{l} \\in \\mathbb{R}^{n}$ and input vector activations from previous layer $a^{l-1} \\in \\mathbb{R}^{n}$\n",
    "\n",
    "So generally for a layer $l$:\n",
    "\n",
    "Pre-activation:\n",
    "$z^{l} = W^{l}a^{l-1}+b^{l}$\n",
    "\n",
    "Post-activation: $a^{l} = \\phi(z^{l})$\n",
    "\n",
    "Shape reminder: if $W^{l}$ is $m * n$ then $a^{l-1}$ must be $n * 1$, producing $z^{l}$ of shape $m * 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5be15",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "Measures how bad a single prediction $\\^{y}$ is compared to the true label y\n",
    "#### Mean squared error (regression)\n",
    "Penalizes larger deviations more heavily, differentiable, maximum likelihhod under Gaussian noise\n",
    "$$\n",
    "L(\\^{y},y) = \\frac{1}{2} \\sum_{i}{}(\\^{y_{i}} - y)^2\n",
    "$$\n",
    "#### Binary classification loss\n",
    "Negative log-likelihood of a Bernoulli distribution, encourage the model to assign high probability to the correct class, penalizes wrong prediction strongly\n",
    "$$\n",
    "L(\\^{y},y) = -(ylog(\\^{y}) + (1-y)log(1-\\^{y}))\n",
    "$$\n",
    "#### Categorical Cross-Entropy (Multi-class classificatio) \n",
    "The target is k classes, represented by 1-hot vector\n",
    "$$\n",
    "L(\\^{y},y) = -\\sum_{j-1}^{K}y_{j}log(\\^{y}_{j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf099c",
   "metadata": {},
   "source": [
    "### Back propagation\n",
    "The above points applies to the step called Forward propagation, since it calculates the values from previous to next neurons\n",
    "\n",
    "Now the core step is to update the model's parameters to align with the ground truth.\n",
    "\n",
    "Thus we should know if we need to increase or decrease a certain neuron's parameter, and how many we should increase/decrease\n",
    "\n",
    "And we should change the parameter for each neuron of each layer $l$, by knowing what are the requirements on the layer $l+1$, so this is the meaning of propagating backwards \n",
    "\n",
    "So from forward propogation, we have these formulas:\n",
    "(Supposing the loss is MSE)\n",
    "$$\n",
    "L(\\^{y},y) = \\frac{1}{2} \\sum_{i}{}(\\^{y_{i}} - y)^2\\\\\n",
    "z = w^{T}x + b = \\sum_{i=1}^{n}w_{i}x_{i} + b\\\\\n",
    "a^{l} = \\phi(z^{l})\n",
    "$$\n",
    "\n",
    "We can cleary notice here that $\\^{y_{i}}$ influences $L(\\^{y},y)$\n",
    "\n",
    "$w^{T}, x, b$ influences $z$\n",
    "\n",
    "$z$ influences $a$\n",
    "\n",
    "What we want to calculate is how much we should increase/decrease the previous layer's neuron\n",
    "\n",
    "Basically we're trying to calculate:\n",
    "$$\\frac{dCost}{dw^{l}}$$\n",
    "\n",
    "The derivative basically tells us the sensitivity of Cost in terms of w, so changing a bit w how much will the cost change?\n",
    "\n",
    "So conceptually since we have the influence change then:\n",
    "$$\n",
    "\\frac{dCost}{dw^{l}} = \\frac{dz^{l}}{dw^{l}}*\\frac{da^{l}}{dz^{l}}*\\frac{dCost}{da^{l}}\n",
    "$$\n",
    "\n",
    "And this is the chain rule in networks\n",
    "\n",
    "Now we can basically calculate all the derivatives, since we have the formula and the all of them are derivable\n",
    "\n",
    "$$\n",
    "\\frac{dz^{l}}{dw^{l}} = a^{l-1}\\\\\n",
    "\\\\\n",
    "\\frac{da^{l}}{dz^{l}} = \\phi^{I}(z^{l})\\\\\n",
    "\\\\\n",
    "\\frac{dCost}{da^{l}} = (a^{l} - y)\n",
    "$$\n",
    "now we can substitute back to original formula:\n",
    "$$\n",
    "\\frac{dCost}{dw^{l}} = a^{l-1}\\phi^{'}(z^{l})(a^{l} - y)\n",
    "$$\n",
    "\n",
    "Since the full cost is averaged between all the examples, also this ratio should be averaged to all training examples\n",
    "\n",
    "To compute the same thing with bias just substitute\n",
    "$$\n",
    "\\frac{dz^{l}}{dw^{l}} to \\frac{dz^{l}}{db^{l}}\\\\\n",
    "\\frac{dCost}{db^{l}} = 1\\phi^{I}(z^{l})(a^{l} - y)\n",
    "$$\n",
    "\n",
    "#### Multiple layer\n",
    "So this was a simplified example, but when we have multiple neurons at each layer?\n",
    "The cost would become:\n",
    "$\\sum_{i}{}(\\^{y_{i}} - y)^2$ and the $z$ would be: $\\sum_{i=1}^{n}w_{i}x_{i} + b$\n",
    "The derivatives are not influenced\n",
    "\n",
    "But in reality we don't use $\\frac{dCost}{dw^{l}}$ but rather the error signal per layer\n",
    "$$\\delta^{l} = \\frac{dL}{dz^{l}} = \\frac{da^{l}}{dz^{l}}⊙\\frac{dCost}{da^{l}}$$\n",
    "\n",
    "So this error signal will be passed during the back propagation, rather recomputing all the previous calculation for each neuron in each layer:\n",
    "\n",
    "$$\n",
    "\\frac{dCost}{dW^{l}} = \\delta^{l}(a^(l-1))^{T}\n",
    "$$\n",
    "\n",
    "here the $W$ indicates the matrix of layer $l$, and for bias:\n",
    "\n",
    "$$\n",
    "\\frac{dCost}{db^{l}} = \\delta^{l}\n",
    "$$\n",
    "\n",
    "We differentiate 2 cases: one for output layer, and the other for the hidden layer\n",
    "\n",
    "Output:\n",
    "$$\n",
    "\\delta^{L} = \\frac{dL}{dz^{l}} = \\frac{dCost}{da^{l}}⊙phi^{'}(z^{l}) = (a^{L} - y)⊙phi^{'}(z^{L})\n",
    "$$\n",
    "\n",
    "Hidden:\n",
    "$$\n",
    "\\delta^{l} = (W^{l+1})^{T}\\delta^{l+1}⊙\\phi^{'}(z^{l})\n",
    "$$\n",
    "\n",
    "Note: $(W^{l+1})^{T}\\delta^{l+1}$ is matrix multiplication and $⊙$ is elementwise multiplication\n",
    "\n",
    "As we can see the $\\delta^{l+1}$ got passed to previous level, so we don't actually need to recompute every derivative each time for each neuron\n",
    "\n",
    "$(W^{l+1})^{T}\\delta^{l+1}$ tells how each neuron in layer $l$ matters to the error in the next layer\n",
    "\n",
    "$\\phi^{'}(z^{l})$ adjusts this by how sensitive the activation is to its input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8ec51",
   "metadata": {},
   "source": [
    "### Update Rules\n",
    "Now that we know \"how sensitive\" are each neuron for adjusting the final result we need to indeed modify and replace them\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "Compute each image in mini-batches, and update in mini-batches\n",
    "$$ W = W - \\alpha\\frac{dL}{dW}$$\n",
    "\n",
    "#### SGD with momentum\n",
    "Don't instantly change direction, but rather buil the momentum\n",
    "$$\n",
    "v_{t} = \\mu v_{t-1} - \\alpha\\frac{d}{d\\Theta_{j}}{L_{t}}\\\\\n",
    "W_{t} = W_{t-1} + v_{t}\n",
    "$$\n",
    "we need to choose the $\\mu = 0.9$ or you can increment it for noisy problems\n",
    "\n",
    "$v$ is the velocity, accumulates a moving average of past fradients\n",
    "\n",
    "$\\mu$ a momentum coefficient\n",
    "\n",
    "$\\frac{d}{d\\Theta_{j}}{L_{t}}$ normal gradient step\n",
    "#### RMSProp (Adaptive learning rate)\n",
    "$$\n",
    "s_{t} = \\Beta s_{t-1} + (1 + \\Beta)(\\frac{d}{d\\Theta_{j}}{L_{t}})^2\\\\\n",
    "W_{t} = W_{t-1} - \\frac{\\alpha}{\\sqrt{s_{t} + \\epsilon}}\\frac{d}{d\\Theta_{j}}{L_{t}}\n",
    "$$\n",
    "\n",
    "$s$: average of squared gradients\n",
    "\n",
    "$\\epsilon = 10^{-8}$: avoid division by 0\n",
    "\n",
    "$\\Beta = 0.9$: Momentum\n",
    "\n",
    "#### Adam\n",
    "A combination of Momentum and RMSProp\n",
    "\n",
    "- Momentum smooths gradients over time\n",
    "\n",
    "- RMSProp rescales learning rate per parameter (large gradient -> smaller)\n",
    "\n",
    "Compute biased first moment (mean of gradients)\n",
    "$$\n",
    "m_{t} = \\Beta_{1}m_{t-1} + (1-\\Beta_{1})Gradient(L_{t})\n",
    "$$\n",
    "Compute biased second moment\n",
    "$$\n",
    "v_{t} = \\Beta_{2}m_{t-1} + (1-\\Beta_{2})Gradient(L_{t})^2\n",
    "$$\n",
    "Bias correction, because at beginning $m_{t},v_{t}$ are too small\n",
    "$$\n",
    "\\^{m_{t}} = \\frac{m_{t}}{1-\\Beta_{1}^{t}}, \\^{v_{t}} = \\frac{v_{t}}{1-\\Beta_{2}^{t}}\n",
    "$$\n",
    "Update the weights and bias\n",
    "$$\n",
    "W_{t} = W_{t-1} - \\alpha\\frac{\\^{m_{t}}}{\\sqrt{\\^{v_{t}}} + \\epsilon}\n",
    "$$\n",
    "$\\Beta_{2} = 0.999$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
