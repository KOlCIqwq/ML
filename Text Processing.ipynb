{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a97311c",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Before doing further analysis we can try to preprocess the text:\n",
    "- Lower-case: convert all the words into lower case (if not matter)\n",
    "- Word stemming: [writing, write, wrote,...] -> write\n",
    "- Lemmatization: group inflected words into a single term\n",
    "- Stop words: common words in a language [I,and,is,...], in most cases they're not needed\n",
    "- Punctuation: sometimes provides useful meanings, and sometimes not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952a1fb",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Consider each word as a token not word.\n",
    "We could have tokens formed by multiple words: [I am] 1 token\n",
    "But for simplicity let's just use lowercase + word = token as tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "273ddcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb36de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog chased the cat\",\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The dog barked loudly\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04fd358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'cat', 'sat', 'on', 'the', 'mat'], ['the', 'dog', 'chased', 'the', 'cat'], ['the', 'cat', 'chased', 'the', 'mouse'], ['the', 'dog', 'barked', 'loudly']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text) -> list:\n",
    "    text = text.lower() # lowercase\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "out = []\n",
    "for phrase in corpus:\n",
    "    out.append(tokenize(phrase))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab624d0a",
   "metadata": {},
   "source": [
    "### Bag of Word (BoW)\n",
    "Build a dictionary of all unique words.\n",
    "Based on a dictionary we encode the phrases into a big array\n",
    "The output array should be the same size of dictionary, but contains how many times it appeared\n",
    "\n",
    "This has 3 downsides: \n",
    "- 2 same BoW can have different meanings\n",
    "    e.g. Alex is smarter than Bob; Bob is smarter than Alex. Same BoW, different meanings\n",
    "- Produce a sparse vector (big array with zero, so meaningless)\n",
    "- Doesn't catch semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbfeede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Build vocabulary, based on their frequencies\n",
    "'''\n",
    "tokenized = []\n",
    "for phrase in corpus:\n",
    "    # Concat\n",
    "    tokenized += tokenize(phrase)\n",
    "# Count frequency\n",
    "counter = Counter(tokenized)\n",
    "sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "w2i = {token: idx for idx, (token, _) in enumerate(sorted_tokens)}\n",
    "\"\"\" w2i[\"UNK\"] = len(sorted_tokens) + 1 \"\"\"\n",
    "vocab = counter.keys()\n",
    "\n",
    "def bow_vec(tokens):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    counter = Counter(tokens) # Process each word\n",
    "    for token,count in counter.items():\n",
    "        if token in w2i:\n",
    "            vec[w2i[token]] = count\n",
    "    return vec\n",
    "\n",
    "example = \"The cat is playing with the dog\"\n",
    "tokens = tokenize(example)\n",
    "bow = bow_vec(tokens)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cea407",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "Instead we can measure the importance of a word in a document\n",
    "\n",
    "Unlike BoW that have meaningless words (like, is, on,...), now we also take consideration about how important each word are\n",
    "$$\n",
    "TF(w,d) = \\frac{count\\ of\\ word\\ (w)\\ in\\ document\\ (d)}{total\\ words\\ in\\ d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75a3210f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28571429 0.14285714 0.14285714 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "def tf(tokens):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    counts = Counter(tokens)\n",
    "    for token,count in counts.items():\n",
    "        if token in w2i:\n",
    "            vec[w2i[token]] = count / len(tokens)\n",
    "        \"\"\" else:\n",
    "            # If unknown then assing it to unkown word in array\n",
    "            vec[w2i[\"UNK\"]] += count / len(tokens) \"\"\"\n",
    "    return vec\n",
    "\n",
    "example = \"The cat is playing with the dog\"\n",
    "tokens = tokenize(example)\n",
    "tf_vec = tf(tokens)\n",
    "print(tf_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936283f0",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "IDF reduces the weight of common words in every document\n",
    "\n",
    "Rare words will produce a high IDF, common will have a low IDF\n",
    "$$\n",
    "IDF(w) = log\\frac{N}{1 + n_{w}}\n",
    "$$\n",
    "where $N$ is number of documents, $n_{w}$ is number of documents containing word w\n",
    "\n",
    "Add 1 to avoid dividing by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "288c56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(corpus_tokens):\n",
    "    N = len(corpus_tokens)\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for token,idx in w2i.items():\n",
    "        contains = sum(1 for doc in corpus_tokens if token in doc)\n",
    "        vec[idx] = np.log((N + 1) / (contains + 1)) + 1 # Smoothed\n",
    "    return vec\n",
    "idf_vec = idf(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674cd95",
   "metadata": {},
   "source": [
    "Now that we have TF and IDF:\n",
    "$TF-IDF(w,d) = TF(w,d) * IDF(w)$\n",
    "For each word, it's weight is determined by:\n",
    "- It's importance in the document (TF)\n",
    "- It's rarity across the corpus (IDF)\n",
    "\n",
    "It fixes few issues of BoW:\n",
    "- Downweights the stopwords\n",
    "- Highlights keywords, unique words will have a higher score\n",
    "- Balnce over documents: long documents don't dominate short ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a115aac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28571429 0.17473479 0.21583223 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tf_vec * idf_vec\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27ae8c",
   "metadata": {},
   "source": [
    "We can also perform a L2 normalization:\n",
    "$$\n",
    "v_{new} = \\frac{v_{i}}{\\sqrt{v_{1}^2+v_{2}^2+...}}\n",
    "$$\n",
    "This makes documents comparable even if they have different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b948255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71709584 0.43855558 0.54170339 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "def l2_normalize(vec):\n",
    "    norm = np.sqrt(np.sum(vec ** 2))\n",
    "    if norm == 0:\n",
    "        return vec  # avoid div by 0\n",
    "    return vec / norm\n",
    "\n",
    "tfidf_normalized = l2_normalize(tfidf_matrix)\n",
    "print(tfidf_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f41268",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "Although we talked about how to represent each word and classify them, they cannot be used to predict the next word\n",
    "\n",
    "The methods above are useful in topic detection, text classification,....\n",
    "\n",
    "So to predict what should the next word be, we should be able to have some relationships between the general text that appeared before prediction and the word predicted\n",
    "\n",
    "To do that "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
